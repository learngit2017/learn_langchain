{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input': 'how I can initialize the model?',\n 'context': [Document(page_content='Model I/O | ü¶úÔ∏èüîó LangChain', metadata={'source': 'https://python.langchain.com/docs/modules/model_io/', 'title': 'Model I/O | ü¶úÔ∏èüîó LangChain', 'description': 'The core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.', 'language': 'en'}),\n  Document(page_content='Skip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsModel I/OOn this pageModel I/OThe core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.QuickstartThe below quickstart will cover the basics of using LangChain\\'s Model I/O components. It will introduce the two different types of models - LLMs and Chat Models. It will then cover how to use Prompt Templates to format the inputs to these models, and how to use Output Parsers to work with the outputs.Language models in LangChain come in two flavors:ChatModels\\u200bChat models are often backed by LLMs but tuned specifically for having conversations.\\nCrucially, their provider APIs use a different interface than pure text completion models. Instead of a single string,\\nthey take a list of chat messages as input and they return an AI message as output. See the section below for more details on what exactly a message consists of. GPT-4 and Anthropic\\'s Claude-2 are both implemented as chat models.LLMs\\u200bLLMs in LangChain refer to pure text completion models.\\nThe APIs they wrap take a string prompt as input and output a string completion. OpenAI\\'s GPT-3 is implemented as an LLM.These two API types have different input and output schemas.Additionally, not all models are the same. Different models have different prompting strategies that work best for them. For example, Anthropic\\'s models work best with XML while OpenAI\\'s work best with JSON. You should keep this in mind when designing your apps.For this getting started guide, we will use chat models and will provide a few options: using an API like Anthropic or OpenAI, or using a local open source model via Ollama.OpenAILocal (using Ollama)Anthropic (chat model only)Cohere (chat model only)First we\\'ll need to install their partner package:pip install langchain-openaiAccessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we\\'ll want to set it as an environment variable by running:export OPENAI_API_KEY=\"...\"We can then initialize the model:from langchain_openai import ChatOpenAIfrom langchain_openai import OpenAIllm = OpenAI()chat_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")If you\\'d prefer not to set an environment variable you can pass the key in directly via the api_key named parameter when initiating the OpenAI LLM class:from langchain_openai import ChatOpenAIllm = ChatOpenAI(api_key=\"...\")Both llm and chat_model are objects that represent configuration for a particular model.\\nYou can initialize them with parameters like temperature and others, and pass them around.\\nThe main difference between them is their input and output schemas.\\nThe LLM objects take string as input and output string.', metadata={'source': 'https://python.langchain.com/docs/modules/model_io/', 'title': 'Model I/O | ü¶úÔ∏èüîó LangChain', 'description': 'The core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.', 'language': 'en'}),\n  Document(page_content=\"This | syntax is powered by the LangChain Expression Language (LCEL) and relies on the universal Runnable interface that all of these objects implement.\\nTo learn more about LCEL, read the documentation here.Conclusion\\u200bThat's it for getting started with prompts, models, and output parsers! This just covered the surface of what there is to learn. For more information, check out:The prompts section for information on how to work with prompt templatesThe ChatModel section for more information on the ChatModel interfaceThe LLM section for more information on the LLM interfaceThe output parser section for information about the different types of output parsers.Help us out by providing feedback on this documentation page:NextPromptsChatModelsLLMsPrompt TemplatesOutput parsersComposing with LCELConclusionCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\", metadata={'source': 'https://python.langchain.com/docs/modules/model_io/', 'title': 'Model I/O | ü¶úÔ∏èüîó LangChain', 'description': 'The core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.', 'language': 'en'}),\n  Document(page_content='The main difference between them is their input and output schemas.\\nThe LLM objects take string as input and output string.\\nThe ChatModel objects take a list of messages as input and output a message.We can see the difference between an LLM and a ChatModel when we invoke it.from langchain_core.messages import HumanMessagetext = \"What would be a good company name for a company that makes colorful socks?\"messages = [HumanMessage(content=text)]llm.invoke(text)# >> Feetful of Funchat_model.invoke(messages)# >> AIMessage(content=\"Socks O\\'Color\")The LLM returns a string, while the ChatModel returns a message.Ollama allows you to run open-source large language models, such as Llama 2, locally.First, follow these instructions to set up and run a local Ollama instance:DownloadFetch a model via ollama pull llama2Then, make sure the Ollama server is running. After that, you can do:from langchain_community.llms import Ollamafrom langchain_community.chat_models import ChatOllamallm = Ollama(model=\"llama2\")chat_model = ChatOllama()Both llm and chat_model are objects that represent configuration for a particular model.\\nYou can initialize them with parameters like temperature and others, and pass them around.\\nThe main difference between them is their input and output schemas.\\nThe LLM objects take string as input and output string.\\nThe ChatModel objects take a list of messages as input and output a message.We can see the difference between an LLM and a ChatModel when we invoke it.from langchain_core.messages import HumanMessagetext = \"What would be a good company name for a company that makes colorful socks?\"messages = [HumanMessage(content=text)]llm.invoke(text)# >> Feetful of Funchat_model.invoke(messages)# >> AIMessage(content=\"Socks O\\'Color\")The LLM returns a string, while the ChatModel returns a message.First we\\'ll need to import the LangChain x Anthropic package.pip install langchain-anthropicAccessing the API requires an API key, which you can get by creating an account here. Once we have a key we\\'ll want to set it as an environment variable by running:export ANTHROPIC_API_KEY=\"...\"We can then initialize the model:from langchain_anthropic import ChatAnthropicchat_model = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=0.2, max_tokens=1024)If you\\'d prefer not to set an environment variable you can pass the key in directly via the api_key named parameter when initiating the Anthropic Chat Model class:chat_model = ChatAnthropic(api_key=\"...\")First we\\'ll need to install their partner package:pip install langchain-cohereAccessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we\\'ll want to set it as an environment variable by running:export COHERE_API_KEY=\"...\"We can then initialize the model:from langchain_cohere import ChatCoherechat_model = ChatCohere()If you\\'d prefer not to set an environment variable you can pass the key in directly via the cohere_api_key named parameter when initiating the Cohere LLM class:from langchain_cohere import ChatCoherechat_model = ChatCohere(cohere_api_key=\"...\")Prompt Templates\\u200bMost LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand.In the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it would be great if the user only had to provide the description of a company/product without worrying about giving the model instructions.PromptTemplates help with exactly this!\\nThey bundle up all the logic for going from user input into a fully formatted prompt.', metadata={'source': 'https://python.langchain.com/docs/modules/model_io/', 'title': 'Model I/O | ü¶úÔ∏èüîó LangChain', 'description': 'The core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.', 'language': 'en'})],\n 'answer': 'To initialize the model, you can follow the instructions provided in the context based on the specific model you are using. For example, if you are using OpenAI, you can initialize the model using the following code:\\n\\n```\\nfrom langchain_openai import ChatOpenAI\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\\n```\\n\\nIf you are using Ollama, you can initialize the model as follows:\\n\\n```\\nfrom langchain_community.llms import Ollama\\nllm = Ollama(model=\"llama2\")\\n```\\n\\nSimilarly, if you are using Anthropic or Cohere models, you can initialize them by following the provided instructions in the context.'}"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# load web resource\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/modules/model_io/\")\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "# create embedding\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# indexing\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: ###{input}###\"\"\")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "retrieval_chain.invoke({\"input\": \"how I can initialize the model?\"})\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T04:25:32.172852Z",
     "start_time": "2024-04-20T04:25:22.944360500Z"
    }
   },
   "id": "612fde62c2a88ac3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
